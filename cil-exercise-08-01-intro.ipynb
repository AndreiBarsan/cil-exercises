{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series 8: Neural Networks feat. TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Session Notes (ETHZ, May 6 2016)\n",
    "\n",
    " * Recommended: Udacity Deep Learning (uses TensorFlow)\n",
    " * No one library to rule them all\n",
    "   * Configuration file-based: Caffe, DistBelief\n",
    "   * Programmatic generation: Torch (Lua), Theano (Python), **TensorFlow** (Python)\n",
    " * Historical HPC progression: Fortran `->` C++ `->` GPU (CUDA, OpenCL)\n",
    " * TensorFlow was chosen for having better support for distributed computation (close call!)\n",
    " * TensorFlow is much slower than e.g. Torch or Theano (as of May 2016)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 62.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# A very simple example of the TensorFlow system.\n",
    "\n",
    "input1 = tf.placeholder(tf.float32, shape=(1))\n",
    "input2 = tf.placeholder(tf.float32, shape=(1))\n",
    "output = tf.mul(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([output], feed_dict={input1: [2], input2: [31]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (Getting Started with TensorFlow)\n",
    "\n",
    "Based on the code skeleton provided by our kind TAs.\n",
    "Note: TF is not designed to handle multiple pipelines in a single\n",
    "file without proper modularization (or interactive sessions). Because of this, it's indicated\n",
    "to just run one of the problems at a time, since otherwise you might\n",
    "stumble upon some weird errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Multi-valued linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some noisy data\n",
    "TRAIN_EXAMPLES = 100000\n",
    "INPUT_DIMENSION = 2\n",
    "OUTPUT_DIMENSION = 3\n",
    "x_data = np.random.rand(TRAIN_EXAMPLES, INPUT_DIMENSION).astype(np.float32)\n",
    "\n",
    "# correct_W = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]]\n",
    "correct_W = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "correct_b = np.array([11, 12, 13], dtype=np.float32)\n",
    "\n",
    "noise_level = 0.01\n",
    "y_data = np.dot(x_data, correct_W) + correct_b + np.random.normal(size=(TRAIN_EXAMPLES, OUTPUT_DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It's better to work with a single large cell, in order\n",
    "# to avoid stale tensors and stuff.\n",
    "\n",
    "def problem_11():\n",
    "\n",
    "    # define the symbolic variables\n",
    "    W = tf.Variable(tf.random_uniform(correct_W.shape, -1.0, 1.0))\n",
    "    b = tf.Variable(tf.zeros(correct_b.shape))\n",
    "\n",
    "    # define the linear model\n",
    "    # y_hat is symbolic!\n",
    "    # 'x_data' is fixed (our data, duh!).\n",
    "    y_hat = tf.matmul(x_data, W) + b\n",
    "\n",
    "    # define the loss\n",
    "    loss = tf.reduce_mean(tf.square(y_hat - y_data))\n",
    "    tf.scalar_summary('log loss', tf.log(1.0 + loss))\n",
    "\n",
    "    # define the optimizer\n",
    "    step_size = 0.1\n",
    "    optimizer = tf.train.GradientDescentOptimizer(step_size)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # initialize the tensorflow session\n",
    "    init = tf.initialize_all_variables()\n",
    "    iterations = 500\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        summary_writer = tf.train.SummaryWriter(\"train/ex1_{}\".format(datetime.datetime.now().strftime(\"%s\")), sess.graph)\n",
    "\n",
    "        # call the train_op many times, each time it will update the variables W and b according to their gradients\n",
    "        for step in range(1, iterations + 1):\n",
    "            _, loss_value, summary_str = sess.run([train_op, loss, summary_op])\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            if step % 100 == 0:\n",
    "                print(\"iteration:\", step, \"loss:\", loss_value)\n",
    "\n",
    "        print(\"learned W:\\n{}\".format(sess.run(W)))\n",
    "        print(\"learned b:\\n{}\".format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Batching and TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def problem_12():\n",
    "    # define the symbolic variables\n",
    "    W = tf.Variable(tf.random_uniform(correct_W.shape, -1.0, 1.0))\n",
    "    b = tf.Variable(tf.zeros(correct_b.shape))\n",
    "    \n",
    "    # define the data placeholders\n",
    "    batch_size = 10\n",
    "    x_ph = tf.placeholder(tf.float32, shape=(batch_size, INPUT_DIMENSION))\n",
    "    y_ph = tf.placeholder(tf.float32, shape=(batch_size, OUTPUT_DIMENSION))\n",
    "\n",
    "    # define the model (using placeholders)\n",
    "    y_hat_batch = tf.matmul(x_ph, W) + b\n",
    "\n",
    "    # define the (stochastic!) loss\n",
    "    loss_batch = tf.reduce_mean(tf.square(y_hat_batch - y_ph))\n",
    "    tf.scalar_summary('log loss', tf.log(1.0 + loss_batch))  # attention: this is the stochastic loss, i.e. it will be noisy\n",
    "\n",
    "    # define the optimizer\n",
    "    step_size = 0.1\n",
    "    optimizer = tf.train.GradientDescentOptimizer(step_size)\n",
    "    train_op = optimizer.minimize(loss_batch)\n",
    "\n",
    "    # initialize the tensorflow session\n",
    "    init = tf.initialize_all_variables()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        summary_writer = tf.train.SummaryWriter(\"train/ex2_{}\".format(datetime.datetime.now().strftime(\"%s\")), sess.graph)\n",
    "\n",
    "    # call the train_op many times, each time it will update the variables W and b according to their gradients\n",
    "        for step in range(5001):\n",
    "            # determine the minibatch\n",
    "            start_index = (batch_size * step) % x_data.shape[0]\n",
    "            stop_index = start_index + batch_size\n",
    "\n",
    "            # get the minibatch data\n",
    "            x_minibatch = x_data[start_index:stop_index]\n",
    "            y_minibatch = y_data[start_index:stop_index]\n",
    "\n",
    "            feed_dict = {\n",
    "                    x_ph: x_minibatch,\n",
    "                    y_ph: y_minibatch\n",
    "                    }\n",
    "\n",
    "            _, loss_value, summary_str = sess.run([train_op, loss_batch, summary_op], feed_dict=feed_dict)\n",
    "            if step % 250 == 0:\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                print(\"iteration:\", step, \"loss:\", loss_value)\n",
    "\n",
    "        print(\"learned W:\\n{}\".format(sess.run(W)))\n",
    "        print(\"learned b:\\n{}\".format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: 226.326\n",
      "iteration: 250 loss: 1.23227\n",
      "iteration: 500 loss: 0.816256\n",
      "iteration: 750 loss: 0.817808\n",
      "iteration: 1000 loss: 1.25341\n",
      "iteration: 1250 loss: 0.767471\n",
      "iteration: 1500 loss: 0.828023\n",
      "iteration: 1750 loss: 0.703546\n",
      "iteration: 2000 loss: 0.924839\n",
      "iteration: 2250 loss: 1.85123\n",
      "iteration: 2500 loss: 1.49962\n",
      "iteration: 2750 loss: 0.580243\n",
      "iteration: 3000 loss: 1.17276\n",
      "iteration: 3250 loss: 1.46334\n",
      "iteration: 3500 loss: 0.717523\n",
      "iteration: 3750 loss: 0.81886\n",
      "iteration: 4000 loss: 0.762826\n",
      "iteration: 4250 loss: 1.01092\n",
      "iteration: 4500 loss: 0.820645\n",
      "iteration: 4750 loss: 0.957444\n",
      "iteration: 5000 loss: 0.894558\n",
      "learned W:\n",
      "[[ 0.87903428  1.90676582  3.0542171 ]\n",
      " [ 3.89054012  4.98080063  6.04779243]]\n",
      "learned b:\n",
      "[ 10.99293709  11.94932175  13.05525112]\n"
     ]
    }
   ],
   "source": [
    "problem_12()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
