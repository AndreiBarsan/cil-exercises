{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike eigendecomposition, can be performed on any matrix $A$, not just on square ones.\n",
    "\n",
    "Exposes useful and interesting properties of $A$:\n",
    "\n",
    " * rank\n",
    " * null-space\n",
    " * orthogonal basis for col/row space\n",
    " \n",
    "Applications of SVD:\n",
    "\n",
    " * computing pseudo-inverse\n",
    " * least-squares fitting of data\n",
    " * matrix approximation\n",
    " * determining rank, range, and null space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![svd](img/svd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of SVD\n",
    "\n",
    "1. $U, V$ orthogonal (rows and cols are orthonormal vectors)\n",
    "2. $U \\in \\mathbb{R}^{M \\times M}, U^TU = UU^T = I$\n",
    "3. $D \\in \\mathbb{M \\times N}$, diagonal\n",
    " * padded with 0-rows at the bottom if M > N\n",
    " * padded with 0-columns right if N > M\n",
    "4. Elements on $\\mathbf{D}$'s diagonal are the **singular values $\\sigma_i$**\n",
    "5. Singular values are ordered from largest to smallest (convention)\n",
    "6. All singular values are positive by convention (TODO(andrei): Is this inherent? Do they get clamped, abs'd?)\n",
    "7. $\\sigma_i = d_{ii} = 0 \\quad \\forall i > \\operatorname{rank}(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If M = N and A is symmetric, then SVD is identical to the eigendecomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit representations of range, null-space, and rank\n",
    "\n",
    " * For the right singular vectors (in V): $\\operatorname{Null}(A) =\n",
    " \\operatorname{ker}(A)  =\n",
    " \\operatorname{span}\\left(\\{ v_i \\> | \\> \\sigma_i = 0 \\}\\right) $\n",
    " * For the left singular vectors (in U): $\\operatorname{Range}(A) =\n",
    " \\operatorname{span}\\left( \\{ u_i \\> | \\> \\sigma_i > 0 \\} \\right)$\n",
    " * $\\operatorname{rank}(A) = $ nr. of non-zero singular values.\n",
    " * $\\operatorname{nullity}(A) = $ nr. of zero singular values.\n",
    " * rank + nullity = min(M, N) (fundamental theorem of LA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): Short note on compact SVD + example from numpy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Approximation via SVD\n",
    "\n",
    "### Frobenius norm\n",
    "\n",
    "$$\n",
    "\\|A\\|_F := \\sqrt{\\sum_{i=1}^M\\sum_{j=1}^Na_{ij}^2} = \\sqrt{\\operatorname{trace}(A^TA)}\n",
    " = \\sqrt{\\sum_{i=1}^K\\sigma_i}, \\quad K = \\min{M, N}\n",
    "$$\n",
    "\n",
    "The frobenius norm can be expressed as the l2 norm of the singular value vector, i.e. the sqrt of the sum of squared singular values.\n",
    "\n",
    "Proof follows from the definition of trace (sum of diagonal elements), and its cyclic nature, $\\operatorname{trace}(XYZ) = \\operatorname{trace}(ZXY)$.\n",
    "\n",
    "TODO(andrei): Why does trace(VD^2V^T) = trace(D^2)?\n",
    "\n",
    "\n",
    "### Singular Values and Matrix Norms\n",
    "\n",
    "Matrix 2-norm = largest singular value:\n",
    "\n",
    "$$ \\| A \\|_2 = \\sup\\left\\{ \\|Ax\\|_2 : \\|x\\|_2 = 1 \\right\\} = \\sigma_1 $$\n",
    "\n",
    "From Wikipedia: In the special case of p = 2 (the Euclidean norm), the induced matrix norm is the spectral norm. The spectral norm of a matrix A is the largest singular value of A i.e. the square root of the largest eigenvalue of the positive-semidefinite matrix $A^TA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eckart-Young Theorem\n",
    "\n",
    "**Reduced SVD** provides an optimal low-rank approximation in the sense of the Frobenius norm.\n",
    "\n",
    "Define:\n",
    "$$\n",
    "\\mathbf{A}_K := \\sum_{i=1}^K \\sigma_i u_i v_i^T, \\quad \\operatorname{rank}(\\mathbf{A}_K )= K\n",
    "$$\n",
    "\n",
    "Then we can find the minimum approximation of $A$, $B$ of rank $K$ as being:\n",
    "\n",
    "$$\n",
    "\\min_{\\operatorname{rank}(B)=K} \\| A - B \\|_F^2 = \\| A - A_K \\|_F^2\n",
    "= \\sum_{k=K+1}^{\\operatorname{rank}(A)}\\sigma_k^2\n",
    "$$\n",
    "\n",
    "In other words, for a fixed rank K approximation, the minimum error we can get is the one obtained by the (rank(A) - K) **smallest eigenvalues**. I.e. the best rank K approximation is given by the K **largest eigenvalues**.\n",
    "\n",
    "It can also be shown that this approximation is not only optimal in the sense of the Frobenius norm, but also in the sense of the **matrix 2-norm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD for Image Compression\n",
    "\n",
    "Represent image as matrix of pixels -> SVD -> compressed (lower-rank) approximation.\n",
    "\n",
    "Q: For a given MxN image, what is the compression ratio, when the compressed image is rank R?\n",
    "\n",
    "Note: following matrices should have a bar (for compact SVD notation).\n",
    "Rank R => kept R singular values => U = M x R, D = R x R, V = R x N.\n",
    "\n",
    "Original = N \\* M\n",
    "\n",
    "Compressed = (M \\* R + R \\* R + R \\* N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full size: 2073600\n",
      "Compressed: 833536\n",
      "Ratio: 40.20% of original\n"
     ]
    }
   ],
   "source": [
    "n = 1080\n",
    "m = 1920\n",
    "R = 256   # Judging by examples in the slides, this should be an OK quality.\n",
    "\n",
    "full_size = n * m\n",
    "compressed_size = m * R + R * R + R * n\n",
    "ratio = compressed_size * 1.0 / full_size\n",
    "print(\"Full size: {0}\\nCompressed: {1}\\nRatio: {2:.2f}% of original\"\n",
    "      .format(full_size, compressed_size, ratio * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition via SVD\n",
    "\n",
    "Columns of U are eigenvectors of $AA^T$.\n",
    "Rows of V are eigenvectors of $A^TA$.\n",
    "\n",
    "If A is symmetrical, then the two are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing SVD\n",
    "\n",
    "![svd-instructions](img/svd-instructions.png)\n",
    "\n",
    "Note: if A symmetric, in the end the resulting $U = AVD^{-1}$ should be the same as the matrix of eigenvectors of $AA^T$. TODO(andrei): Double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): Small primer on how to compute eigenpairs for small matrices.\n",
    "# Necessary for exam! (Good reference available on Khan Academy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
