{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Mixture Models and the Expectation Maximization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means review\n",
    "\n",
    "K-means optimizes the cost function $J$, where\n",
    "\n",
    "$$\n",
    "J(U, Z) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{k,n} \\| x_n - u_k \\|_2^2, \\quad\n",
    "\\text{s.t.} \\> z_{k,n} \\in \\{0, 1\\} \\land \\sum_{k=1}{K} z_{k,n} = 1 \\> \\forall n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want **probabilistic cluster assignments**. Instead of saying \"this datapoint belongs to that cluster\", we want to say that \"this data point belongs to cluster X with probability 80%, cluster Y with probability 15%, and cluster Z with probability 5%\".\n",
    "\n",
    "**Relax z's constraint** to $z_{k,n} \\in [0, 1]$, i.e. let more than one $z_{\\cdot,n}$ be non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **generative model == statistical model** and infer its parameters $\\theta \\in \\Theta$ using **Maximum Likelihood**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML = pick model under which data has highest likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta; \\mathbf{X}) := p_{\\theta}(\\mathbf{X}) \\overset{\\text{i.i.d.}}{=}\n",
    "\\prod_{n=1}^N p_{\\theta}(x_n)\n",
    "$$\n",
    "\n",
    "We choose the **maximum likelihood estimator** $\\hat{\\theta}$ which maximizes the likelihood of the data, whose data points we assumed to be independently sampled from the same underlying distribution:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = {\\arg\\max}_{\\theta\\in\\Theta}p_{\\theta}(\\mathbf{X}) = \n",
    "{\\arg\\max}_{\\theta\\in\\Theta} \\sum_{n=1}^{N} \\log p_{\\theta}(\\mathbf{x_n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture models\n",
    "\n",
    "Finite mixture model for one data point:\n",
    "\n",
    "$$\n",
    "p_\\theta(x) = \\sum_{k=1}^K \\pi_k p_{\\theta_k}(x)\n",
    "$$\n",
    "\n",
    "Mixing proportions which sum up to 1. Every distribution has a proportion, **not every data point**. The whole model contains just $K$ mixing proportions. They dictate the relative cluster sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model (GMM)\n",
    "\n",
    "$p_{\\theta_k}(x) = \\mathcal{N}(\\mu_k, \\Sigma_k)$\n",
    "\n",
    "For clustering, two-stage sampling. Sample a cluster, then sample from that cluster's gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce hard assignment variables $z_k$, BUT we denote their assignment probabilities by $\\pi_k$, and work with those:\n",
    "\n",
    "$$ P(z_k = 1) = \\pi_k $$\n",
    "\n",
    "$$ p_\\pi(z) = \\prod_{k=1}^{K} \\pi_k^{z_k} $$\n",
    "\n",
    "(this multiplies only the probability of the kth row of the n column; all the rest are zero)\n",
    "\n",
    "We are now left with the following joint distribution over x and z (**complete data** distribution):\n",
    "\n",
    "$$\n",
    "p_\\theta(x, z) = \\prod_{k=1}^K \\left[ \\pi_k p_{\\theta_k}(x) \\right]^{z_k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final MLE objective: \n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = {\\arg\\max}_{\\theta\\in\\Theta} \\sum_{n=1}^{N} \\log p_{\\theta}(\\mathbf{x_n})\n",
    " = {\\arg\\max}_{\\theta\\in\\Theta} \\sum_{n=1}^{N} \\log \\left[ \\sum_{k=1}^K \\pi_k p_{\\theta_k}(x_n) \\right]\n",
    "$$\n",
    "\n",
    "Hard to optimize directly. No closed-form solution (problem sum inside the log)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation maximization\n",
    "\n",
    "Cannot optimize log-likelihood directly (contains sum of logs). Instead we can maximize a lowe bound on the log-likelihood, based on the complete data distribution, $p_\\theta(x, y)$.\n",
    "\n",
    "### Jensen's inequality\n",
    "\n",
    "A secant line of a convex function is always above the graph.\n",
    "\n",
    "$$ \\log\\left( \\frac{ \\sum_{i=1}^n x_i }{n} \\right) \\ge \\frac{\\sum_{i=1}^n\\log{x_i} }{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Expectation Step\n",
    "\n",
    "Optimize **bound** w.r.t. \"helper\" distribution $q$.\n",
    "\n",
    "TODO(andrei): Upgrade these sections based on the Bishop book since the slides aren't great.\n",
    "\n",
    "$$ \\sum_{k=1}^K q_k \\le 1 \\quad \\forall k $$\n",
    "\n",
    "Lagrangian for each data point, obtain optimal q for each data point. Fix this and perform next step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Maximization step\n",
    "\n",
    "Can establish closed-form solutions for $\\mathbf{\\mu}^*$ and $\\mathbf{\\Sigma}^*$ given the previous $q$s, as well as the data.\n",
    "\n",
    " * c.f. the centroid position recomputation in K-means. Remember that here, we don't have just ONE cluster assignment per point, but we have soft assignments to ALL clusters.\n",
    " * c.f. naive derivation of EM solution (see slides and Bishop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): Table/diagram with precise comparison between K-means and EM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "**We still need to pick K first, even in GMMs solved with the EM algorithm!!!**\n",
    "\n",
    "One can technically keep increasing K until K==N and the LL of the data given the model keeps getting better. This is not good!\n",
    "\n",
    "### AIC and BIC\n",
    "\n",
    "$\\kappa(\\cdot)$ = number of free parameters in model.\n",
    "\n",
    "So what does a model contain? Assuming full covariance matrices:\n",
    "K \\* D means, and K \\* (D + 1) \\* D \\* 1/2 covariances PLUS K - 1 weights (-1 because we're talking about free variables, and we know they must sum to 1) $\\pi$.\n",
    "\n",
    "Note that a covariance matrix is symmetric!\n",
    "\n",
    "#### Akaike Information Criterion\n",
    "\n",
    "(Smaller is better)\n",
    "\n",
    "$$ AIC(\\theta | X) = -\\log p_\\theta(X) + \\kappa(\\theta) $$\n",
    "\n",
    "#### Bayesian Information Criterion\n",
    "\n",
    "(Smaller is better)\n",
    "\n",
    "$$ BIC(\\theta | X) = -\\log p_\\theta(X) + \\frac{1}{2} \\kappa(\\theta) \\log N $$\n",
    "\n",
    "BIC is harsher.\n",
    "\n",
    "#### Howto\n",
    "\n",
    "Both AIC and BIC have clear minimum when computed for multiple Ks. It tends to coincide with the knee in the LL decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
