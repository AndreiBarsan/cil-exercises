{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Mixture Models and the Expectation Maximization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means review\n",
    "\n",
    "K-means optimizes the cost function $J$, where\n",
    "\n",
    "$$\n",
    "J(U, Z) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{k,n} \\| x_n - u_k \\|_2^2, \\quad\n",
    "\\text{s.t.} \\>\\> z_{k,n} \\in \\{0, 1\\} \\land \\sum_{k=1}{K} z_{k,n} = 1 \\> \\forall n\n",
    "$$\n",
    "\n",
    "Formulated as a matrix factorization, we have:\n",
    "\n",
    "$$\n",
    "\\min_{U,Z} J(U, Z) = \\min_{U,Z} \\left\\| X - UZ \\right\\|_F^2 \n",
    "\\overset{(1)}{=} \\min_UZ \\sum_{n=1}^N \\left\\| x_n - \\sum_{k=1}^K z_{kn} u_k \\ \\right\\|_2^2\n",
    "$$\n",
    "\n",
    "**(1) Tip:** We can decompose the squared Frobenius norm into squared row l2 norms, since all we're doing is adding element squares together. Note that if the norms weren't squared, we wouldn't be able to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want **probabilistic cluster assignments**. Instead of saying \"this datapoint belongs to that cluster\", we want to say that \"this data point belongs to cluster X with probability 80%, cluster Y with probability 15%, and cluster Z with probability 5%\".\n",
    "\n",
    "**Relax z's constraint** to $z_{k,n} \\in [0, 1]$, i.e. let more than one $z_{\\cdot,n}$ be non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilisitic Clustering\n",
    "\n",
    "Use **generative model == statistical model** and infer its parameters $\\theta \\in \\Theta$ using **Maximum Likelihood**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML = pick model under which data has highest likelihood.\n",
    "\n",
    "\n",
    "**Probabilities**: Parameter given $\\implies$ data to be **generated**\n",
    " \n",
    "**Likelihood**: Outcome (data) given $\\implies$ parameter to be **inferred**\n",
    "\n",
    "We turn the probability of observed data under $\\theta$ *into* a likelihood function for $\\theta$, given the outcome.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta; \\mathbf{X}) := p_{\\theta}(\\mathbf{X}) \\overset{\\text{i.i.d.}}{=}\n",
    "\\prod_{n=1}^N p_{\\theta}(x_n)\n",
    "$$\n",
    "\n",
    "We choose the **maximum likelihood estimator** $\\hat{\\theta}$ which maximizes the likelihood of the data, whose data points we assumed to be independently sampled from the same underlying distribution:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = {\\arg\\max}_{\\theta\\in\\Theta}p_{\\theta}(\\mathbf{X}) = \n",
    "{\\arg\\max}_{\\theta\\in\\Theta} \\sum_{n=1}^{N} \\log p_{\\theta}(\\mathbf{x_n})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture models\n",
    "\n",
    "Finite mixture model for one data point:\n",
    "\n",
    "$$\n",
    "p_\\theta(x) = \\sum_{k=1}^K \\pi_k p_{\\theta_k}(x)\n",
    "$$\n",
    "\n",
    "Mixing proportions which sum up to 1. Every distribution has a proportion, **not every data point**. The whole model contains just $K$ mixing proportions. They dictate the relative cluster sizes.\n",
    "\n",
    "GMMs are a special case where $\\theta_k = \\left( \\mu_k, \\Sigma_k \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model (GMM)\n",
    "\n",
    "$$p_{\\theta_k}(x) = \\mathcal{N}(\\mu_k, \\Sigma_k) \\implies \n",
    "p_{\\theta}(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "For clustering, two-stage sampling. Sample a cluster, then sample from that cluster's gaussian.\n",
    "\n",
    "ô°€Cluster index k: **latent variable**.\n",
    "\n",
    "Final outcome x: **observed**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a *graphical model* where we factor $p(x)$, the likelihood of a data point, as $p(x) = \\sum_z p(x | z) p(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce hard assignment variables $z_k$, BUT we denote their assignment probabilities by $\\pi_k$, and work with those:\n",
    "\n",
    "$$ P(z_k = 1) = \\pi_k $$\n",
    "\n",
    "$$ p_\\pi(z) = p_\\pi(z_1, \\dots, z_K) = \\prod_{k=1}^{K} \\pi_k^{z_k} $$\n",
    "\n",
    "Since the assignment z only has a 1 on one position by definition, the above product is actually always guaranteed to have just one term.\n",
    "\n",
    "$$\n",
    "p(x | z_k = 1) = \\mathcal{N}(x | \\mu_k, \\Sigma_k ) \\iff\n",
    "p(x | z ) = \\prod_{k=1}^K \\mathcal{N}(x | \\mu_k, \\Sigma_k )^{z_k}\n",
    "$$\n",
    "\n",
    "Same as above; the second way of writing has just one term in practice.\n",
    "\n",
    "We are now left with the following joint distribution over x and z (**complete data** distribution):\n",
    "\n",
    "$$\n",
    "p_\\theta(x, z) \n",
    "\\overset{\\text{joint dist.}}{=} p_\\theta(x | z) p_\\theta(z) \n",
    "= \\prod_{k=1}^K \\mathcal{N}(x | \\mu_k, \\Sigma_k )^{z_k} \\prod_{k=1}^{K} \\pi_k^{z_k}\n",
    "= \\prod_{k=1}^K \\left[ \\pi_k p_{\\theta_k}(x) \\right]^{z_k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also marginalize over z:\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_z p(x, z) = \\sum_z p(z) p(x | z) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k) \\overset{\\text{notation}}{=} \\sum_{k=1}^K \\pi_k p_{\\theta_k}(x)\n",
    "$$\n",
    "\n",
    "We see that for every observed data point $x_n$, there exists a latent variable (vector) $z_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressing the probability of an assignment\n",
    "\n",
    "We denote $p(z_k = 1 | x) =: \\gamma(z_k)$. We can compute a solution for $\\gamma(z_k)$ using Bayes' theorem. $\\gamma(z_k)$ is the posterior probability, after we have observed x.\n",
    "\n",
    "$$\n",
    "\\gamma(z_k) = p(z_k = 1 \\> | \\> x) = \\frac{p(x | z_k = 1) p(z_k = 1)}{\\sum_{j=1}^{K} p(x | z_j = 1) p(z_j = 1)}\n",
    "= \\frac{\\mathcal{N}(x|\\mu_k, \\Sigma_k) \\pi_k}{\\sum_{j=1}^K \\mathcal{N}(x|\\mu_j, \\Sigma_j) \\pi_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final MLE objective: \n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = {\\arg\\max}_{\\theta\\in\\Theta} \\sum_{n=1}^{N} \\log p_{\\theta}(\\mathbf{x_n})\n",
    " = {\\arg\\max}_{\\theta\\in\\Theta} \\sum_{n=1}^{N} \\log \\left[ \\sum_{k=1}^K \\pi_k p_{\\theta_k}(x_n) \\right]\n",
    "$$\n",
    "\n",
    "Hard to optimize directly. No closed-form solution because of the summation over k inside the logarithm, which makes the logarithm no longer act directly on the Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with ML\n",
    "\n",
    "When applying ML to GMMs, **singularities** can occur, and pose a problem.\n",
    "\n",
    "If we end up, at some point, with a mean $\\mu_k$ very close or equal to a data point $x_n$, then the point's contribution to the likelihood function would look like ($\\Sigma_k := \\sigma_k^2I$ for simplicity):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{N}(x_n | \\mu_k, \\Sigma_k) & =\n",
    "\\left( 2 \\pi \\right)^{-\\frac{d}{2}} |\\Sigma_k|^{-\\frac{1}{2}} \\exp\\left( -\\frac{1}{2} (x_n - \\mu_j)^T \\Sigma_k^{-1} (x_n - \\mu_j) \\right) \\\\\n",
    " & = \\left( 2 \\pi \\right)^{-\\frac{d}{2}} (\\sigma_k^2)^{-\\frac{1}{2}} \\exp(0) \\\\\n",
    " & = \\frac{c}{\\sigma_k} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\sigma_k \\to 0 $, then the point's contribution to the likelihood becomes infinite, together with the whole log likelihood. The maximization of the likelihood is therefore not a well posed problem, and special heuristics need to be used, such as resetting a component's mean and covariance whenever it becomes problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation maximization\n",
    "\n",
    "Cannot optimize log-likelihood directly (contains sum of logs). Instead we can maximize a lowe bound on the log-likelihood, based on the complete data distribution, $p_\\theta(x, y)$.\n",
    "\n",
    "### Jensen's inequality\n",
    "\n",
    "A secant line of a convex function is always above the graph.\n",
    "\n",
    "$$ \\log\\left( \\frac{ \\sum_{i=1}^n x_i }{n} \\right)\n",
    "= \\log\\left(\\sum_{i=1}^n \\frac{1}{n} x_i \\right) \n",
    "\\ge\n",
    "= \\sum_{i=1}^n \\frac{1}{n} \\log x_i \n",
    "= \\frac{\\sum_{i=1}^n\\log{x_i} }{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Expectation Step\n",
    "\n",
    "TODO(andrei): See how useful the part with Jensen's inequality really is.\n",
    "Optimize (maximize) **lower bound** w.r.t. \"helper\" distribution $q$.\n",
    "\n",
    "Evaluate the responsabilities using the current parameter values.\n",
    "\n",
    "$$\n",
    "\\gamma(z_{kn}) = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j) }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Maximization step\n",
    "\n",
    "Can establish closed-form solutions for $\\mathbf{\\mu}^*$ and $\\mathbf{\\Sigma}^*$ given the previous $\\gamma$s, as well as the data:\n",
    "\n",
    "$$\n",
    "\\mu_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{kn}) x_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{kn}) (x_n - \\mu_k^{\\text{new}})(x_n - \\mu_k^{\\text{new}})^T\n",
    "$$\n",
    "\n",
    "The cluster weight $\\pi_k$ is proportional to how much responsability a cluster assumes overall\n",
    "\n",
    "$$\n",
    "\\pi_k^{\\text{new}} = \\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "N_k = \\sum_{n=1}^N \\gamma(z_{nk})\n",
    "$$\n",
    "\n",
    " * c.f. the centroid position recomputation in K-means. Remember that here, we don't have just ONE cluster assignment per point, but we have soft assignments to ALL clusters.\n",
    " * c.f. naive derivation of EM solution (see slides and Bishop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): Table/diagram with precise comparison between K-means and EM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "**We still need to pick K first, even in GMMs solved with the EM algorithm!!!**\n",
    "\n",
    "One can technically keep increasing K until K==N and the LL of the data given the model keeps getting better. This is not good!\n",
    "\n",
    "### AIC and BIC\n",
    "\n",
    "$\\kappa(\\cdot)$ = number of free parameters in model.\n",
    "\n",
    "So what does a model contain? Assuming full covariance matrices:\n",
    "K \\* D means, and K \\* (D + 1) \\* D \\* 1/2 covariances PLUS K - 1 weights (-1 because we're talking about free variables, and we know they must sum to 1) $\\pi$.\n",
    "\n",
    "Note that a covariance matrix is symmetric!\n",
    "\n",
    "#### Akaike Information Criterion\n",
    "\n",
    "(Smaller is better)\n",
    "\n",
    "$$ AIC(\\theta | X) = -\\log p_\\theta(X) + \\kappa(\\theta) $$\n",
    "\n",
    "#### Bayesian Information Criterion\n",
    "\n",
    "(Smaller is better)\n",
    "\n",
    "$$ BIC(\\theta | X) = -\\log p_\\theta(X) + \\frac{1}{2} \\kappa(\\theta) \\log N $$\n",
    "\n",
    "BIC is harsher.\n",
    "\n",
    "#### Howto\n",
    "\n",
    "Both AIC and BIC have clear minimum when computed for multiple Ks. It tends to coincide with the knee in the LL decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
