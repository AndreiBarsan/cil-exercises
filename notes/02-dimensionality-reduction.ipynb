{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main goal\n",
    "\n",
    "Finding information-preserving or \"interesting\" projections from high-dimensional feature space to low-dimensional space.\n",
    "\n",
    "### Applications\n",
    " * compression (loss: reconstruction error)\n",
    " * feature selection (loss: classification/generalization error)\n",
    " * complexity reduction\n",
    " * signal recovery (noise removal)\n",
    " * data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "The following serve to showcase the **intrinsic lower-dimensionality** of high dimensional data (non-obvious at first glance, at least to me).\n",
    "\n",
    "Most high-dimensional data (e.g. full-HD images of cats in $\\mathbb{R}^{6,220,800}$) actually lie on a lower-dimensional non-linear manifold in this high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Simple generative model\n",
    "\n",
    "Look at an artifical dataset's pairwise distances as a function of the **dataset dimensionality**.\n",
    "\n",
    "Hint: know the properties of Gaussians, and know them well!\n",
    "\n",
    "Assume data vector $x = (x_1, \\dots, x_D)^\\top, \\> x_d \\sim \\mathcal{N}(0, 1)$ (standard normal component distribution).\n",
    "\n",
    "This means:\n",
    "\n",
    "$$\\mathbf{x}, \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "Applying the difference/sum properties of gaussian-distributed RVs, we get:\n",
    "\n",
    "$$ \\mathbf{x} - \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, 2\\mathbf{I}) \\iff \n",
    "\\delta = \\frac{1}{2}(\\mathbf{x} - \\mathbf{y}) \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \n",
    "$$\n",
    "\n",
    "The equivalence holds because x and y are independent, and in that case the variance is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Definition:** The chi-squared distribution with k degrees of freedom is the distribution of a sum of the squares of k independent **standard normal** random variables.\n",
    "\n",
    "Squaring the previous difference, we get the following, knowing that the squared norm of a multivariate normal RV is $\\chi^2(D)$-distributed (properties of Gaussians; $D$ is the dimensionality of our data):\n",
    "\n",
    "$$ \\frac{1}{2} \\| \\mathbf{x} - \\mathbf{y} \\|^2 \\sim \\chi^2(D) $$\n",
    "\n",
    "Since our difference variable had a $\\sigma^2$ of $2\\mathbf{I}$, we want to halve that, hence the $\\frac{1}{2}$, since the definition of the $\\chi^2$ distribution only applies to **standard normal** distributions (0 mean, unit variance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\chi^2$ is a special case of the $\\Gamma$ distribution:\n",
    "\n",
    "$$ \\chi^2(D) = \\Gamma\\left(\\frac{D}{2}, 2\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the average squared difference between x and y (per dimension). Practically speaking, this is just a scaled version of the halved norm ($\\delta$).\n",
    "\n",
    "$$\\Delta(x, y) := \\frac{1}{D}\\sum_{d=1}^{D}(x_d - y_d)^2 = \\frac{2}{D}\\delta $$\n",
    "\n",
    "We know that $\\delta$ is a chi-squared RV. We scale that and get the following ([properties of $\\Gamma$ RVs](https://en.wikipedia.org/wiki/Gamma_distribution#Scaling); yeah, I don't know them by heart either :/):\n",
    "\n",
    "$$\\delta \\sim \\chi^2(D) \\implies \\Delta(x, y) = \\frac{2}{D}\\chi^2(D) \\sim \\Gamma\\left( \\frac{D}{2}, \\frac{2}{D} \\cdot 2 \\right) = \\Gamma\\left( \\frac{D}{2}, \\frac{4}{D} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know how the average pairwise distance between two datapoints is distributed. \n",
    "\n",
    "Let's plot that as a function of $D$, the dimensionality of our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and variance as a function of D (using definitions from Wikipedia).\n",
    "\n",
    "$$ \\mathbb{E}[\\Delta] = k\\theta = 2 $$\n",
    "$$ \\mathbb{V}[\\Delta] = \\frac{D}{2}\\left(\\frac{4}{D}\\right)^2 = \\frac{8}{D} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the variance shrinks as the dimensionality $D$ of our data increases!\n",
    "\n",
    "**Result: we now have a way to veryify whether the intrinsic dimensionality of the data is actually the same as our real dimensionality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): Plot this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oil data: D = 12, Gamma-dist fit is OK, intrinsic dimensionality seems to match real one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion capture data: D = 102, no fit; intrinsic dimensionality must be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): Read corresponding section in Elements of Statistical Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): StackOverflow answer comparing OLS and PCA.\n",
    "# TLDR: OLS computes distances perpendicular to axes.\n",
    "#       PCA computes distances perpendiculat to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Project data onto $K \\le D$ dimensional space while maximizing variance of the projected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few slides start from this goal, and show that **the optimal solution (which maximizes variance) is achieved via the eigendecomposition of the data matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective A: Variance maximization\n",
    "\n",
    "#### K = 1 (First principal direction)\n",
    "\n",
    "Start with just one component, $u_1$. Unit length, since length doesn't matter.\n",
    "\n",
    "Mean of projected data $u_1^T\\bar{x}$ ($\\bar{x}$ is the sample mean).\n",
    "\n",
    "Variance of projected data: $u_1^T\\Sigma u_1$. This is what we want to maximize.\n",
    "\n",
    "$$ \\max_{u_1} u_1^T \\Sigma u_1, \\quad \\text{s.t.} \\quad \\|u_1\\|_2=1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a constrained optimization problem. To incorporate the constraint, we write its Lagrangian:\n",
    "\n",
    "$$ \\mathcal{L} := u_1^T \\Sigma u_1 + \\lambda \\left(1 - \\|u_1\\|^2_2\\right) = u_1^T \\Sigma u_1 + \\lambda \\left(1 - u_1^Tu_1 \\right)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial{\\mathbf{u}_1}}\\mathcal{L} \\overset{!}{=} \\iff 2\\Sigma u_1 - 2\\lambda u_1 = 0 \\iff \\Sigma u_1 = \\lambda u_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this look familiar?\n",
    "\n",
    "The solutions to this equation are the eigenpairs of the $\\Sigma$ matrix!\n",
    "\n",
    "TODO(andrei): Explain this bullshit!\n",
    "\n",
    "$\\lambda$ is the variance of the projected data (WHY?): $\\lambda = u_1^T \\Sigma u_1$.\n",
    "\n",
    "To maximize the variance, we just have to pick the eigenvector with the largest eigenvalue! It is called the **principal direction**.\n",
    "\n",
    "TODO(andrei): Plain English explanation of whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 2 (Second Principal Direction)\n",
    "\n",
    "Want to find $u_2$ s.t. $u_2^Tu_1 = 0$ that maximizes the variance $ = u_2^T \\Sigma u_2$.\n",
    "\n",
    "TODO(andrei): Do we assume u_1 is fixed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, we optimize for the best $u_2$ by writing the Lagrangian.\n",
    "\n",
    "$$ \n",
    "\\mathcal{L} = u_2^T \\Sigma u_2 + \\lambda\\left(1 - u_2^T u_2 \\right) + \n",
    "\\eta(u_2^T u_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial u_2}\\mathcal{L} = 2 \\Sigma u_2 - 2\\lambda u_2 + \\eta u_1\n",
    "\\overset{!}{=} 0\n",
    "$$\n",
    "\n",
    "TODO(andrei): Why is it allowed to first omit the second constraint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective B: Error minimization\n",
    "\n",
    "(shown to be formally equivalent to objective A)\n",
    "\n",
    "Therefore, maximizing variance and minimizing error $\\|x - \\tilde{x}\\|_2$ lead to the same solution, which is awesome for us! Our two main objectives do not diverge! (Which would have required painful tradeoffs.)\n",
    "\n",
    "WHY THE FLIP DO THE SLIDE NOT CONTAIN THIS BEAUTIFUL CONCLUSION EXPLICITLY? I know it was likely stated in the lecture (haven't checked physical notes yet) and at the beginning of the process, but this is just silly. It should be between slides 26 and 27..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as a matrix factorization\n",
    "\n",
    "Represent data as matrix: **columns are data points**, rows are features.\n",
    "\n",
    " 1. Mean-center data.\n",
    " 2. Compute eigenvalue spectrum.\n",
    " 3. Take eigenvectors with K highest values into matrix $U_K$. \n",
    " 4. Project X onto the space spanned by the eigenvectors and get approximation $Z_K$.\n",
    " 5. K = D $\\implies$ perfect reconstruction. **PCA is a Matrix Factorization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to pick a good K? Look for knee in eigenvalue spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
