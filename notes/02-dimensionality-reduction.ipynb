{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "### Main goal\n",
    "\n",
    "Finding information-preserving or \"interesting\" projections from high-dimensional feature space to low-dimensional space.\n",
    "\n",
    "### Applications\n",
    " * compression (loss: reconstruction error)\n",
    " * feature selection (loss: classification/generalization error)\n",
    " * complexity reduction\n",
    " * signal recovery (noise removal)\n",
    " * data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "The following serve to showcase the **intrinsic lower-dimensionality** of high dimensional data (non-obvious at first glance, at least to me).\n",
    "\n",
    "Most high-dimensional data (e.g. full-HD images of cats in $\\mathbb{R}^{6,220,800}$) actually lie on a lower-dimensional non-linear manifold in this high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Simple generative model\n",
    "\n",
    "Look at an artifical dataset's pairwise distances as a function of the **dataset dimensionality**.\n",
    "\n",
    "Hint: know the properties of Gaussians, and know them well!\n",
    "\n",
    "Assume data vector $x = (x_1, \\dots, x_D)^\\top, \\> x_d \\sim \\mathcal{N}(0, 1)$ (standard normal component distribution).\n",
    "\n",
    "This means:\n",
    "\n",
    "$$\\mathbf{x}, \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "Applying the difference/sum properties of gaussian-distributed RVs, we get:\n",
    "\n",
    "$$ \\mathbf{x} - \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, 2\\mathbf{I}) \\iff \n",
    "\\delta = \\frac{1}{2}(\\mathbf{x} - \\mathbf{y}) \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \n",
    "$$\n",
    "\n",
    "The equivalence holds because x and y are independent, and in that case the variance is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Definition:** The chi-squared distribution with k degrees of freedom is the distribution of a sum of the squares of k independent **standard normal** random variables.\n",
    "\n",
    "Squaring the previous difference, we get the following, knowing that the squared norm of a multivariate normal RV is $\\chi^2(D)$-distributed (properties of Gaussians; $D$ is the dimensionality of our data):\n",
    "\n",
    "$$ \\frac{1}{2} \\| \\mathbf{x} - \\mathbf{y} \\|^2 \\sim \\chi^2(D) $$\n",
    "\n",
    "Since our difference variable had a $\\sigma^2$ of $2\\mathbf{I}$, we want to halve that, hence the $\\frac{1}{2}$, since the definition of the $\\chi^2$ distribution only applies to **standard normal** distributions (0 mean, unit variance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\chi^2$ is a special case of the $\\Gamma$ distribution:\n",
    "\n",
    "$$ \\chi^2(D) = \\Gamma\\left(\\frac{D}{2}, 2\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the average squared difference between x and y (per dimension). Practically speaking, this is just a scaled version of the halved norm ($\\delta$).\n",
    "\n",
    "$$\\Delta(x, y) := \\frac{1}{D}\\sum_{d=1}^{D}(x_d - y_d)^2 = \\frac{2}{D}\\delta $$\n",
    "\n",
    "We know that $\\delta$ is a chi-squared RV. We scale that and get the following ([properties of $\\Gamma$ RVs](https://en.wikipedia.org/wiki/Gamma_distribution#Scaling); yeah, I don't know them by heart either :/):\n",
    "\n",
    "$$\\delta \\sim \\chi^2(D) \\implies \\Delta(x, y) = \\frac{2}{D}\\chi^2(D) \\sim \\Gamma\\left( \\frac{D}{2}, \\frac{2}{D} \\cdot 2 \\right) = \\Gamma\\left( \\frac{D}{2}, \\frac{4}{D} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know how the average pairwise distance between two datapoints is distributed. \n",
    "\n",
    "Let's plot that as a function of $D$, the dimensionality of our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and variance as a function of D (using definitions from Wikipedia).\n",
    "\n",
    "$$ \\mathbb{E}[\\Delta] = k\\theta = 2 $$\n",
    "$$ \\mathbb{V}[\\Delta] = \\frac{D}{2}\\left(\\frac{4}{D}\\right)^2 = \\frac{8}{D} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the variance shrinks as the dimensionality $D$ of our data increases!\n",
    "\n",
    "**Result: we now have a way to veryify whether the intrinsic dimensionality of the data is actually the same as our real dimensionality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): Plot this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oil data: D = 12, Gamma-dist fit is not horrible, but not good either (higher STD than theory would otherwise indicate), but intrinsic dimensionality is still smaller (2 degrees of freedom for gas-water-oil mixture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion capture data: D = 102, no fit; intrinsic dimensionality must be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): Read corresponding section in Elements of Statistical Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): StackOverflow answer comparing OLS and PCA.\n",
    "# TLDR: OLS computes distances perpendicular to axes.\n",
    "#       PCA computes distances perpendiculat to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Project data onto $K \\le D$ dimensional space while maximizing variance of the projected data.\n",
    "\n",
    "PCA seeks a space of lower dimensionality, known as the **principal subspace**, such as the **orthogonal** projection of the data points onto this subspace **maximizes the variance** of the projected points.\n",
    "\n",
    "PCA minimizes the sum-of-squares of the projection errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few slides start from this goal, and show that **the optimal solution (which maximizes variance) is achieved via the eigendecomposition of the data matrix.**\n",
    "\n",
    "Our approach has **two** main objective: ensuring that the variance of the lower-dimensional data is maximal, and that the  projection error is minimal. The following two sections, A and B, will show that these objectives are actually equivalent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective A: Variance maximization\n",
    "\n",
    "#### K = 1 (First principal direction)\n",
    "\n",
    "Start with just one component, $u_1$. Unit length, since length doesn't matter.\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{N} \\sum_{n=1}^N (x_n - \\bar{x}) (x_n - \\bar{x})^T\n",
    "$$\n",
    "\n",
    "Mean of projected data: $u_1^T\\bar{x}$ ($\\bar{x}$ is the sample mean).\n",
    "\n",
    "Proof:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{n=1}^N u_1^T x_n = u_1 \\sum_{n=1}^n x_n = u_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "Variance of projected data: $u_1^T\\Sigma u_1$. (Similar proof.)\n",
    "\n",
    "The variance is what we want to maximize:\n",
    "\n",
    "$$ u_1^{*} = \\max_{u_1} u_1^T \\Sigma u_1, \\quad \\text{s.t.} \\quad \\|u_1\\|_2=1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the constraint on the magnitude of $u_1$ is necessary, since otherwise the maximization would be trivial with $\\| u_1 \\|_2 \\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a constrained optimization problem. To incorporate the constraint, we write its Lagrangian:\n",
    "\n",
    "$$ \\mathcal{L} := u_1^T \\Sigma u_1 + \\lambda \\left(1 - \\|u_1\\|^2_2\\right) = u_1^T \\Sigma u_1 + \\lambda \\left(1 - u_1^Tu_1 \\right)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial{\\mathbf{u}_1}}\\mathcal{L} \\overset{!}{=} \\iff 2\\Sigma u_1 - 2\\lambda u_1 = 0 \\iff \\Sigma u_1 = \\lambda u_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this look familiar?\n",
    "\n",
    "The solutions to this equation are the eigenpairs of the $\\Sigma$ matrix!\n",
    "\n",
    "TODO(andrei): Explain this better. The formula for the variance comes from applying $u_1x_n$ to the formula for the variance of $X$.\n",
    "\n",
    "$\\lambda$ is the variance of the projected data: $\\lambda = u_1^T \\Sigma u_1$.\n",
    "\n",
    "To maximize the variance, we just have to pick the eigenvector with the largest eigenvalue! It is called the **principal direction**:\n",
    "\n",
    "$$\\Sigma u_1 = \\lambda u_1 \\iff u_1^T \\Sigma u_1 = \\lambda u_1^T u_1 \\overset{u_1^T u_1 = 1}{\\iff} u_1^T \\Sigma u_1 = \\lambda\n",
    "$$\n",
    "\n",
    "The variance ($=u_1^T \\Sigma u_1$) is thus maximized when we pick the largest eigenvalue $\\lambda$.\n",
    "\n",
    "Plain English explanation: Starting off with projecting onto just one direction for simplicity, we write the variance maximization objective. We then write its Lagrangian, since it's a constrained objective, and setting the Lagrangian's gradient to zero leads us to a closed form solution: the eigenvector equation itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 2 (Second Principal Direction)\n",
    "\n",
    "Want to find $u_2$ s.t. $u_2^Tu_1 = 0$ that maximizes the variance $ = u_2^T \\Sigma u_2$.\n",
    "\n",
    "We do this incrementally. We assume prevoius principal components are fixed, and choose new directions which maximize the projected variance **among all possible directions orthogonal to those already considered**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, we optimize for the best $u_2$ by writing the Lagrangian.\n",
    "\n",
    "$$ \n",
    "\\mathcal{L} = u_2^T \\Sigma u_2 + \\lambda\\left(1 - u_2^T u_2 \\right) + \n",
    "\\eta(u_2^T u_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial u_2}\\mathcal{L} = 2 \\Sigma u_2 - 2\\lambda u_2 + \\eta u_1\n",
    "\\overset{!}{=} 0\n",
    "$$\n",
    "\n",
    "TODO(andrei): Why is it allowed to first omit the second constraint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective B: Error minimization\n",
    "\n",
    "(shown to be formally equivalent to objective A)\n",
    "\n",
    "(show \"Thus any eigenvector will define a stationary point of the distortion measure.\")\n",
    "\n",
    "It can be shown that the distortion J for projecting down to K dimensions is:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=K+1}^D \\lambda_i\n",
    "$$\n",
    "\n",
    "The distortion, which depends on all the components we \"cut off\", is minimized by choosing the smallest D - K eigenvalues. We should therefore choose the largest eigenvalues to project on! And this solution is equivalent to the one reached by optimizing objective A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Therefore, maximizing variance and minimizing error $\\|x - \\tilde{x}\\|_2$ lead to the same solution, which is awesome for us! Our two main objectives do not diverge! (Which would have required painful tradeoffs.)\n",
    "\n",
    "**The sum of the discarded eigenvalues is equal to the sum of squared differences between the points and their projections.** This is fucking beautiful!\n",
    "\n",
    "WHY THE FLIP DO THE SLIDE NOT CONTAIN THIS BEAUTIFUL CONCLUSION EXPLICITLY? I know it was likely stated during the lecture (haven't checked physical notes yet) and at the beginning of the process, but this is just silly. It should be between slides 26 and 27..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as a matrix factorization\n",
    "\n",
    "Represent data as matrix: **columns are data points**, rows are features.\n",
    "\n",
    " 1. Mean-center data.\n",
    " 2. Compute eigenvalue spectrum.\n",
    " 3. Take eigenvectors with K highest values into matrix $U_K$. \n",
    " 4. Project X onto the space spanned by the eigenvectors and get approximation $Z_K$.\n",
    " 5. K = D $\\implies$ perfect reconstruction. **PCA is a Matrix Factorization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to pick a good K? Look for knee in eigenvalue spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Eigenpairs and eigendecomposition\n",
    "\n",
    "(Mostly taken from Appendix C of Bishop's PRML.)\n",
    "\n",
    "The eigendecomposition $A = U\\Lambda U^T$ of a matrix arises naturally from (a) first selecting a set of orthonormal eigenvectors and (b) generalizing the eigenvector equation for all eigenvalues in matrix notation:\n",
    "\n",
    "Let $ U \\in \\mathbb{K \\times K} $ be a matrix whose columns are orthonormal eigenvectors of a matrix $A$ of rank $K$. (A is a square $K\\times K$ matrix.)\n",
    "\n",
    "For the orthonormal matrix $U$ it holds that $UU^T = I \\land U^TU = I \\iff U^T = U^-1$.\n",
    "\n",
    "We can then \"stack\" all the eigenvector equations as\n",
    "\n",
    "$$ Au_i = \\lambda_i u_i \\mapsto AU = U \\Lambda \\iff\n",
    "AUU^T = U \\Lambda U^T \\iff A = U \\Lambda U^T\n",
    "$$\n",
    "\n",
    "Whereby $U$ can be interpreted as a rigid rotation of the coordinate system (vectors aren't scaled and all angles remain unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
