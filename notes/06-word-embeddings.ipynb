{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > The meaning of a word is its use in the language.\n",
    " \n",
    "(Wittgenstein, 1953)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Learn word representations that capture word **meanings** based on examples in a corpus.\n",
    "\n",
    "Embed words in a **vector space**.\n",
    "\n",
    "\n",
    "## Models\n",
    "\n",
    "### Skip-gram model\n",
    "\n",
    "Use a word to predict its context. P(w1|w2) = prob that w1 occurs in the vicinity of w2.\n",
    "\n",
    "Objective:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta; \\mathbf{w}) = \\sum_{t=1}^T \\sum_{\\Delta \\in \\mathcal{I}} \n",
    "\\log p_\\theta \\left(w^{(t+\\Delta)} \\> | \\> w^{(t)} \\right).\n",
    "$$\n",
    "\n",
    "For every word in the corpus (index $t$), go through its neighbors and maximize the likelihood of the neighbor given the source word.\n",
    "\n",
    "Use MLE and compute $\\hat{\\theta} = {\\arg\\!\\max}_\\theta \\mathcal{L}(\\theta; \\mathbf{w})$.\n",
    "\n",
    "However, we haven't yet described what (1) $w_t$ actually is, and (2) how to model $p_\\theta(w \\> | \\> w')$. \n",
    "\n",
    "\n",
    "### Continuous bag-of-words\n",
    "\n",
    "Use context to predict word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent vector model\n",
    "\n",
    " 1. Map word as vector + bias $ \\> \\in \\mathbb{R}^{D+1}$.\n",
    " 2. Define **log-bilinear model** for the likelihoods:\n",
    " \n",
    " $$\\log p_\\theta(w \\> | \\> w') = \\langle x_{w}, x_{w'} \\rangle + b_w + \\text{const.}$$\n",
    "\n",
    "\n",
    "Bilinear because it's contains two linear components.\n",
    "\n",
    "Why is the constant necessary? It's the normalizer denominator. +constant because constant can be negative, because we're inside log, so we can take log of a/b as log(a) - log(b).\n",
    "\n",
    "**Key insight in representation: **\n",
    "\n",
    "$$ \\angle(x_w, x_{w'}) \\downarrow \\quad \\implies \\quad p_\\theta(w | w') \\uparrow $$\n",
    "\n",
    "The higher the cosine similarity (and lower the angle between the words), the higher the likelihood of the log-bilinear model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screw it let's see some pre-trained ones first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# This can be downloaded from:\n",
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "pretrained_w2v_file = '../../project/data/word2vec/GoogleNews-vectors-negative300.bin'\n",
    "w2v = Word2Vec.load_word2vec_format(fname=pretrained_w2v_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.6958590149879456),\n",
       " ('kings', 0.5950952768325806),\n",
       " ('queens', 0.5838501453399658),\n",
       " ('monarch', 0.5398427248001099),\n",
       " ('prince', 0.5223615169525146),\n",
       " ('princess', 0.5175285339355469),\n",
       " ('princes', 0.49844634532928467),\n",
       " ('royal', 0.4924592971801758),\n",
       " ('NYC_anglophiles_aflutter', 0.4859851002693176),\n",
       " ('Eugene_Ionesco_absurdist_comedy', 0.4784241318702698)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['queen', 'man'], negative=['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(andrei): T-SNE visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with current model\n",
    "\n",
    "1. bilinearity\n",
    "2. Huge denominator, summing over entire vocabulary for **every word.**\n",
    "\n",
    "## Solving the problems\n",
    "\n",
    "1. Context vectors -> two different embeddings, vector and context embeddings. Makes model more flexible at the cost of complexity.\n",
    "\n",
    "2. Don't use maximum likelihood. In our case, we use weighted square loss and GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the partition function? Used to refer to the denominator for probability normalization. Can e.g. skip it in GloVe by computing $\\tilde{p}(w)$ instead of $\\tilde{p}$, which still works reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GloVe vs. word2vec\n",
    "\n",
    "| GloVe | word2vec \n",
    "---|---|---\n",
    " Computation | Requires precomputed co-occurrence matrix. | Requires repeated iterations through entire corpus\n",
    " Model | Contrastive divergence  | Weighted squared loss\n",
    "| Count-based model which essentially tries to factorize the co-occurrence matrix.  | Predictive model; goal is to improve (lower) the loss of predicting the target words from the context words given the vector representations. Keep updating vector representations via SGD to achieve this like in any learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the difference between normalized and unnormalized GloVe?\n",
    "\n",
    "| Normalized | Unnormalized\n",
    "---|---|---\n",
    " | Requires computation of partition function (normalization constant) | No need for computing partition function.\n",
    " | Large \"spikes\" (large $h(w)$) counterbalanced by normalization | No implicit counterbalancing; need extra attention\n",
    " \n",
    "TODO(andrei): What is a two-sided loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe uses an unnormalized probability distribution $\\tilde{p}_{\\theta}$.\n",
    "\n",
    "$$\\tilde{p}_\\theta(w_i \\> | \\> w_j) = \\exp\\left[ \\langle x_i, y_i \\rangle + b_i + d_j \\right]\n",
    "\\iff\n",
    "\\exp\\left[ \\langle \\bar{x_i}, \\bar{y_i} \\rangle \\right]\n",
    "$$\n",
    "\n",
    "Provided we augment $x$ and $y$ as follows:\n",
    "\n",
    "$$ \n",
    "D := D + 2; \\quad\n",
    "\\bar{x}_{w,D-1} = 1, \\> \\bar{x}_{w,D} = b_w;\n",
    "\\bar{y}_{w,D-1} = d_w, \\> \\bar{y}_{w,D} = 1;\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define $M$ and stack the word vectors $x_w$ and the context vectors $y_w$ into the matrices $X$ and $Y$:\n",
    "\n",
    "$$ M = (m_{i,j}), m_{i,j} = \\log n_{i,j} $$\n",
    "\n",
    "$$\n",
    "X := \\left[ x_{w_1} \\dots x_{w_{|\\mathcal{V}\\,|}} \\right], \\quad\n",
    "Y := \\left[ y_{w_1} \\dots y_{w_{|\\mathcal{C}|}} \\right]\n",
    "$$\n",
    "\n",
    "(Remember, $\\mathcal{V}$ represents the vocabulary, and $\\mathcal{C}$ represents the contexts.)\n",
    "\n",
    "\n",
    "Using these nifty tricks, we can now show that...\n",
    "\n",
    "## GloVe solves a matrix factorization problem\n",
    "\n",
    "IFF $f := 1$:\n",
    "\n",
    "$$ \\min_{X,Y} \\| M - X^T Y \\|_F^2 $$\n",
    "\n",
    "Since we try to compute X and Y such as their product (pairwise inner products between col vectorss in X and col vectors in Y are as close as possible to the logs in $M$).\n",
    "\n",
    "TODO(andrei): This was covered in the exercise session and the homework. Put the proof here and explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
