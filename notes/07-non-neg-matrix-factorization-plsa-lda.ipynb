{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic models\n",
    "\n",
    "Find low-dimensional document representation in topic/concept space.\n",
    "Approach: **predictive model** (log-likelihood)\n",
    "\n",
    "But: can't actually use ML. Instead, we maximize a lower bound on the likelihood using expectation maximization, like we did in GMMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pLSA\n",
    "\n",
    "### Sampling model\n",
    "\n",
    "**Topic-wise sampling.** For every token:\n",
    " (1) Sample topic from categorical distribution.\n",
    " (2) Sample word from that category.\n",
    " \n",
    "Generally:\n",
    "\n",
    "$$\n",
    "p(w|d) = \\sum_{z=1}^{K} p(w|z) p(z|d)\n",
    "$$\n",
    "\n",
    "(where z is the topic; need to pre-specify K)\n",
    "\n",
    "How do we reach this? Start with prob. of word given document, and use sum rule t (law of total probability) to express it as a sum over $z$:\n",
    "\n",
    "$$\n",
    "p(w | d) = \\sum_{z} p( w, z \\>|\\> d ) = \\sum_{z} p ( w \\>|\\> z, d ) p ( z \\>|\\> d )\n",
    "$$\n",
    "\n",
    "Remember the graphical model! The way we constructed it, we assume that knowing the topic is **enough** to predict a word:\n",
    "\n",
    "$$ w \\perp d  \\>|\\> z \\iff p(w|z, d) = p(w|z) $$\n",
    "\n",
    "We can apply this in the above formulation as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "p(w | d) = \\sum_{z} p ( w \\>|\\> z, d ) p ( z \\>|\\> d )\n",
    " = \\sum_{z} p(w \\>|\\> z) p (z \\>|\\> d)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we compute these distributions?\n",
    "\n",
    "Assume words are C.I., apply log, and get log-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(U, V) \\overset{\\text{c.i.}}{=} \\sum_{i,j} x_{ij} \\log p(w_j \\>|\\> d_i)\n",
    " = \\sum_{(i, j) \\in \\mathcal{X}} \\log \\sum_{z=1}^{K} p(w_j \\>|\\> z) p(z \\>|\\> d_i)\n",
    " := \\sum_{(i, j) \\in \\mathcal{X}} \\log \\sum_{z=1}^{K} v_{zj} u_{zi}\n",
    "$$\n",
    "\n",
    "Where: $u_{zi} \\ge 0 \\land \\sum_z u_zi = 1$, i.e. for a given document, the sum of its components it 1. Should not be interpreted probabilistically, as a document **can** cover multiple topics.\n",
    "\n",
    "And: $v_{zj} \\ge 0 \\land \\sum_j v_zj = 1$, i.e. for a given topic z, its words sum up to 1, i.e. a topic is a probability distribution of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the log likelihood\n",
    "\n",
    "We have a sum of logs, so we can't directly (just like in MMs), but we **can** optimize  a lower bound instead:\n",
    "\n",
    "$$\n",
    "\\underbrace{\n",
    "\\log\\sum_{z=1}^K q_{zij} \\frac{u_{zi} v_{zj}}{q_{zij}}\n",
    "}_{\\substack{\\text{Our objective} \\\\ \\text{Hard to optimize because of the log(sum).}}}\n",
    "\\overset{\\text{Jensen's inequality}}{\\ge}\n",
    "\\sum_{z=1}^K q_{zij} \\log \\frac{u_{zi} v_{zj}}{q_{zij}}\n",
    "=\n",
    "\\underbrace{\n",
    "\\sum_{z=1}^K q_{zij} \\left[ \\log u_{zi} + \\log v_{zj} - \\log q_{zij} \\right]\n",
    "}_{\\substack{\\text{\n",
    "Lower bound on our objective.}\\\\ \\text{Not perfect, but solvable} \\\\ \\text{(logarithm now inside sum)}\n",
    "}}\n",
    "$$\n",
    "\n",
    "If we maximize the lower bound well enough, we can get very close to our real objective, which we cannot optimize directly. So, handwavily, given that we're going\n",
    "\"in the right direction\", we can get away with computing the optimal model parameters\n",
    "by optimizing the lower bound instead of the main objective.\n",
    "\n",
    "TODO(andrei): Theory on how good the approximation actually is.\n",
    "\n",
    "So why is a $\\log\\left(\\sum\\right)$ hard to optimize? Well, its derivative ends up having the whole sum as a denominator, preventing it from being decomposable via addition. This makes SGD impossible, which is really bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation step\n",
    "\n",
    "Optimize $q$:\n",
    "\n",
    "$$\n",
    "q_{zij} = \\frac{v_{zj}u_{zi}}{\\sum_{k=1}^K v_{kj} u_{ki}} \n",
    " = \\frac{  p(w_j | z) p(z | d_i) }{\\sum_{k=1}^K p(w_j | k) p(k | d_i)}\n",
    "$$\n",
    "\n",
    "$q_{\\cdot i j}$ now contains the posterior of the occurrence (i, j) at the current step, i.e. given the previous U and V matrices. In the next step, we'll update the matrices themselves based on the newly-computed $q$s.\n",
    "\n",
    "Yes, this is a closed-form solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximization step\n",
    "\n",
    "Compute optimal U and V based on the $q$s determined in the previous expectation-step. Like in K-means, where we update the model itself, i.e. the centroid positions, based on the latent variable, i.e. the assignments.\n",
    "\n",
    "U, which models the topics in documents.\n",
    "\n",
    "$$\n",
    "u_{zi} = \\frac{\\sum_j x_{ij} q_{zij}}{\\sum_j x_{ij}}\n",
    "$$\n",
    "\n",
    "V, which models the word distributions in topics.\n",
    "\n",
    "$$\n",
    "v_{zj} = \\frac{\\sum_i x_{ij} q_{zij}}{\\sum_{i,l} x_{il} q_{zil}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Start with generative document model. For this, we first need to sample the topic weights $u_i$ for the new documents.\n",
    "\n",
    "I.e. we need to come up with a probability distribution from which we can sample a probability vector.\n",
    "\n",
    "While $u$ shouldn't be interpreted as a probabilty vector qualitatively, since it's meant to describe a mixture of topics in a document $d_i$, rather then the likelihood of that docuent belonging to certain topics, it still showcases the main properties of a categorical probability distribution: every element is $\\ge 0$ and they all sum up to 1.\n",
    "\n",
    "So mathematically, we can still treat $u_i$ as a probability vector.\n",
    "\n",
    "And we sample it from a **Dirichlet distribution**, which is the **conjugate distribution** of the categorical distribution. [When in doubt, Wiki it!](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions)\n",
    "\n",
    "We therefore model $u_i$ sampled from a Dirichled distribution, parameterized by $\\alpha$, which is the number of parameters in $u$, i.e. the number of topics ($\\alpha = K$):\n",
    "\n",
    "$$\n",
    "p(\\mathbf{u}_i \\>|\\> \\alpha) \\propto \\prod_{z=1}^K u_{zi}^{\\alpha_k-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat U as nuisance parameter (TODO(andrei): WHY?).\n",
    "\n",
    "Each column of U represents a document, and every row of that column represents a certain topic $z$'s weight in that document.\n",
    "\n",
    "Why are V the only real parameters?\n",
    "TODO(andrei): Ask around and/or check out Bishop book or Andrew Ng's paper on LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x|V, u) = \\frac{l!}{\\prod_j x_j!}\\prod_j \\pi_j^{x_j}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "\\pi_j := \\sum_z v_{zj} u_{z}\n",
    "$$\n",
    "\n",
    "What is $pi_j$ for a word $j$? \n",
    "\n",
    "Bayesian averaging integrates over $u$, conditioning on $\\alpha$, the nr. of categories. WHAT DOES THIS DO?\n",
    "\n",
    "Learning LDA: **beyond the scope of the lecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization\n",
    "\n",
    "Given a non-negative integer count matrix $X \\in \\mathbb{Z}_{\\ge 0}^{N\\times M}$.\n",
    "\n",
    "NMF computes:\n",
    "\n",
    "$$\n",
    "X \\approx U^T V\n",
    "$$\n",
    "\n",
    "Where U and V are non-negative, end every column has a sum of 1. \n",
    "\n",
    "NMF objective:\n",
    "\n",
    "$$\n",
    "J(U,V) = \\frac{1}{2} \\| X - U^T V \\|^2_F \\quad \\text{s.t.} \\> u_{zi}, v_{zj} \\ge 0\n",
    "$$\n",
    "\n",
    "(fraction meant to lead to cleaner gradient)\n",
    "\n",
    "So we optimize:\n",
    "\n",
    "$$\n",
    "U^{*}, V^{*} = \\min_{U,V} J(U, V)\n",
    "$$\n",
    "\n",
    "Objective convex in U and in V, but not jointly in U and V at the same time, so we use **alternating least squares (ALS)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
