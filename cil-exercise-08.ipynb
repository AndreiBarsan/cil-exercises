{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series 8: Neural Networks feat. TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (Getting Started with TensorFlow)\n",
    "\n",
    "Based on the code skeleton provided by our kind TAs.\n",
    "Note: TF is not designed to handle multiple pipelines in a single\n",
    "file without proper modularization. Because of this, it's indicated\n",
    "to just run one of the problems at a time, since otherwise you might\n",
    "stumble upon some weird errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Multi-valued linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some noisy data\n",
    "TRAIN_EXAMPLES = 100000\n",
    "INPUT_DIMENSION = 2\n",
    "OUTPUT_DIMENSION = 3\n",
    "x_data = np.random.rand(TRAIN_EXAMPLES, INPUT_DIMENSION).astype(np.float32)\n",
    "\n",
    "# correct_W = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]]\n",
    "correct_W = [[1, 2, 3], [4, 5, 6]]\n",
    "correct_b = [11, 12, 13]\n",
    "correct_W, correct_b = map(lambda l: np.array(l, dtype=np.float32), (correct_W, correct_b))\n",
    "\n",
    "noise_level = 0.01\n",
    "y_data = np.dot(x_data, correct_W) + correct_b + np.random.normal(size=(TRAIN_EXAMPLES, OUTPUT_DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It's better to work with a single large cell, in order\n",
    "# to avoid stale tensors and stuff.\n",
    "\n",
    "def problem_11():\n",
    "\n",
    "    # define the symbolic variables\n",
    "    W = tf.Variable(tf.random_uniform(correct_W.shape, -1.0, 1.0))\n",
    "    b = tf.Variable(tf.zeros(correct_b.shape))\n",
    "\n",
    "    # define the linear model\n",
    "    # y_hat is symbolic!\n",
    "    # 'x_data' is fixed (our data, duh!).\n",
    "    y_hat = tf.matmul(x_data, W) + b\n",
    "\n",
    "    # define the loss\n",
    "    loss = tf.reduce_mean(tf.square(y_hat - y_data))\n",
    "    tf.scalar_summary('log loss', tf.log(1.0 + loss))\n",
    "\n",
    "    # define the optimizer\n",
    "    step_size = 0.1\n",
    "    optimizer = tf.train.GradientDescentOptimizer(step_size)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # initialize the tensorflow session\n",
    "    init = tf.initialize_all_variables()\n",
    "    iterations = 500\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        summary_writer = tf.train.SummaryWriter(\"train/ex1_{}\".format(datetime.datetime.now().strftime(\"%s\")), sess.graph)\n",
    "\n",
    "        # call the train_op many times, each time it will update the variables W and b according to their gradients\n",
    "        for step in range(1, iterations + 1):\n",
    "            _, loss_value, summary_str = sess.run([train_op, loss, summary_op])\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            if step % 100 == 0:\n",
    "                print(\"iteration:\", step, \"loss:\", loss_value)\n",
    "\n",
    "        print(\"learned W:\\n{}\".format(sess.run(W)))\n",
    "        print(\"learned b:\\n{}\".format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Batching and TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def problem_12():\n",
    "    # define the symbolic variables\n",
    "    W = tf.Variable(tf.random_uniform(correct_W.shape, -1.0, 1.0))\n",
    "    b = tf.Variable(tf.zeros(correct_b.shape))\n",
    "    \n",
    "    # define the data placeholders\n",
    "    batch_size = 10\n",
    "    x_ph = tf.placeholder(tf.float32, shape=(batch_size, INPUT_DIMENSION))\n",
    "    y_ph = tf.placeholder(tf.float32, shape=(batch_size, OUTPUT_DIMENSION))\n",
    "\n",
    "    # define the model (using placeholders)\n",
    "    y_hat_batch = tf.matmul(x_ph, W) + b\n",
    "\n",
    "    # define the (stochastic!) loss\n",
    "    loss_batch = tf.reduce_mean(tf.square(y_hat_batch - y_ph))\n",
    "    tf.scalar_summary('log loss', tf.log(1.0 + loss_batch))  # attention: this is the stochastic loss, i.e. it will be noisy\n",
    "\n",
    "    # define the optimizer\n",
    "    step_size = 0.1\n",
    "    optimizer = tf.train.GradientDescentOptimizer(step_size)\n",
    "    train_op = optimizer.minimize(loss_batch)\n",
    "\n",
    "    # initialize the tensorflow session\n",
    "    init = tf.initialize_all_variables()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        summary_writer = tf.train.SummaryWriter(\"train/ex2_{}\".format(datetime.datetime.now().strftime(\"%s\")), sess.graph)\n",
    "\n",
    "    # call the train_op many times, each time it will update the variables W and b according to their gradients\n",
    "        for step in range(201):\n",
    "            # determine the minibatch\n",
    "            start_index = (batch_size * step) % x_data.shape[0]\n",
    "            stop_index = start_index + batch_size\n",
    "\n",
    "            # get the minibatch data\n",
    "            x_minibatch = x_data[start_index:stop_index]\n",
    "            y_minibatch = y_data[start_index:stop_index]\n",
    "\n",
    "            feed_dict = {\n",
    "                    x_ph: x_minibatch,\n",
    "                    y_ph: y_minibatch\n",
    "                    }\n",
    "\n",
    "            _, loss_value, summary_str = sess.run([train_op, loss_batch, summary_op], feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            print(\"iteration:\", step, \"loss:\", loss_value)\n",
    "\n",
    "        print(\"learned W:\\n{}\".format(sess.run(W)))\n",
    "        print(\"learned b:\\n{}\".format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: 256.021\n",
      "iteration: 1 loss: 205.065\n",
      "iteration: 2 loss: 155.628\n",
      "iteration: 3 loss: 136.074\n",
      "iteration: 4 loss: 113.611\n",
      "iteration: 5 loss: 87.1557\n",
      "iteration: 6 loss: 71.481\n",
      "iteration: 7 loss: 49.5685\n",
      "iteration: 8 loss: 49.4222\n",
      "iteration: 9 loss: 37.0111\n",
      "iteration: 10 loss: 30.8024\n",
      "iteration: 11 loss: 23.6687\n",
      "iteration: 12 loss: 18.7336\n",
      "iteration: 13 loss: 16.8241\n",
      "iteration: 14 loss: 16.3116\n",
      "iteration: 15 loss: 14.3766\n",
      "iteration: 16 loss: 12.3003\n",
      "iteration: 17 loss: 7.79278\n",
      "iteration: 18 loss: 7.31078\n",
      "iteration: 19 loss: 7.60889\n",
      "iteration: 20 loss: 6.23099\n",
      "iteration: 21 loss: 4.25003\n",
      "iteration: 22 loss: 2.84943\n",
      "iteration: 23 loss: 2.57385\n",
      "iteration: 24 loss: 2.33063\n",
      "iteration: 25 loss: 3.71958\n",
      "iteration: 26 loss: 3.6538\n",
      "iteration: 27 loss: 2.03445\n",
      "iteration: 28 loss: 2.46061\n",
      "iteration: 29 loss: 3.12589\n",
      "iteration: 30 loss: 1.49019\n",
      "iteration: 31 loss: 1.86478\n",
      "iteration: 32 loss: 2.24545\n",
      "iteration: 33 loss: 1.40993\n",
      "iteration: 34 loss: 2.17417\n",
      "iteration: 35 loss: 2.8454\n",
      "iteration: 36 loss: 1.37918\n",
      "iteration: 37 loss: 1.36681\n",
      "iteration: 38 loss: 1.42466\n",
      "iteration: 39 loss: 1.78394\n",
      "iteration: 40 loss: 1.52768\n",
      "iteration: 41 loss: 1.47552\n",
      "iteration: 42 loss: 1.17004\n",
      "iteration: 43 loss: 1.99894\n",
      "iteration: 44 loss: 1.85113\n",
      "iteration: 45 loss: 1.93899\n",
      "iteration: 46 loss: 1.72776\n",
      "iteration: 47 loss: 1.78185\n",
      "iteration: 48 loss: 1.79805\n",
      "iteration: 49 loss: 1.65984\n",
      "iteration: 50 loss: 2.46476\n",
      "iteration: 51 loss: 2.51796\n",
      "iteration: 52 loss: 1.61948\n",
      "iteration: 53 loss: 1.35045\n",
      "iteration: 54 loss: 1.03874\n",
      "iteration: 55 loss: 1.69087\n",
      "iteration: 56 loss: 1.38776\n",
      "iteration: 57 loss: 1.45008\n",
      "iteration: 58 loss: 1.21371\n",
      "iteration: 59 loss: 1.64748\n",
      "iteration: 60 loss: 1.72908\n",
      "iteration: 61 loss: 2.1067\n",
      "iteration: 62 loss: 2.94972\n",
      "iteration: 63 loss: 2.29561\n",
      "iteration: 64 loss: 1.25831\n",
      "iteration: 65 loss: 1.5777\n",
      "iteration: 66 loss: 2.58676\n",
      "iteration: 67 loss: 1.34626\n",
      "iteration: 68 loss: 1.12817\n",
      "iteration: 69 loss: 2.25281\n",
      "iteration: 70 loss: 1.87872\n",
      "iteration: 71 loss: 1.81573\n",
      "iteration: 72 loss: 1.44223\n",
      "iteration: 73 loss: 0.699081\n",
      "iteration: 74 loss: 1.10816\n",
      "iteration: 75 loss: 1.30373\n",
      "iteration: 76 loss: 1.55366\n",
      "iteration: 77 loss: 1.12349\n",
      "iteration: 78 loss: 2.41406\n",
      "iteration: 79 loss: 1.42387\n",
      "iteration: 80 loss: 1.45201\n",
      "iteration: 81 loss: 1.48258\n",
      "iteration: 82 loss: 1.57542\n",
      "iteration: 83 loss: 1.37714\n",
      "iteration: 84 loss: 1.27596\n",
      "iteration: 85 loss: 1.10303\n",
      "iteration: 86 loss: 1.99328\n",
      "iteration: 87 loss: 1.55735\n",
      "iteration: 88 loss: 1.56165\n",
      "iteration: 89 loss: 2.22709\n",
      "iteration: 90 loss: 1.35211\n",
      "iteration: 91 loss: 0.882539\n",
      "iteration: 92 loss: 1.81362\n",
      "iteration: 93 loss: 2.21458\n",
      "iteration: 94 loss: 1.21813\n",
      "iteration: 95 loss: 1.85962\n",
      "iteration: 96 loss: 1.47997\n",
      "iteration: 97 loss: 1.19404\n",
      "iteration: 98 loss: 1.25277\n",
      "iteration: 99 loss: 1.39953\n",
      "iteration: 100 loss: 1.09344\n",
      "iteration: 101 loss: 1.06552\n",
      "iteration: 102 loss: 1.41391\n",
      "iteration: 103 loss: 0.910237\n",
      "iteration: 104 loss: 0.670551\n",
      "iteration: 105 loss: 1.19043\n",
      "iteration: 106 loss: 0.722464\n",
      "iteration: 107 loss: 1.88826\n",
      "iteration: 108 loss: 1.44413\n",
      "iteration: 109 loss: 1.28528\n",
      "iteration: 110 loss: 1.34208\n",
      "iteration: 111 loss: 1.02854\n",
      "iteration: 112 loss: 1.0254\n",
      "iteration: 113 loss: 0.515346\n",
      "iteration: 114 loss: 1.07237\n",
      "iteration: 115 loss: 2.24448\n",
      "iteration: 116 loss: 0.946932\n",
      "iteration: 117 loss: 1.58967\n",
      "iteration: 118 loss: 0.759628\n",
      "iteration: 119 loss: 1.22904\n",
      "iteration: 120 loss: 0.723711\n",
      "iteration: 121 loss: 2.37527\n",
      "iteration: 122 loss: 1.80681\n",
      "iteration: 123 loss: 1.01979\n",
      "iteration: 124 loss: 1.93296\n",
      "iteration: 125 loss: 1.50002\n",
      "iteration: 126 loss: 1.01108\n",
      "iteration: 127 loss: 1.02136\n",
      "iteration: 128 loss: 1.481\n",
      "iteration: 129 loss: 1.10553\n",
      "iteration: 130 loss: 1.48149\n",
      "iteration: 131 loss: 1.90969\n",
      "iteration: 132 loss: 2.47828\n",
      "iteration: 133 loss: 1.16792\n",
      "iteration: 134 loss: 1.79158\n",
      "iteration: 135 loss: 1.58244\n",
      "iteration: 136 loss: 0.972405\n",
      "iteration: 137 loss: 1.81403\n",
      "iteration: 138 loss: 1.49614\n",
      "iteration: 139 loss: 1.7355\n",
      "iteration: 140 loss: 1.17847\n",
      "iteration: 141 loss: 1.37716\n",
      "iteration: 142 loss: 1.04505\n",
      "iteration: 143 loss: 1.26028\n",
      "iteration: 144 loss: 1.32509\n",
      "iteration: 145 loss: 1.06709\n",
      "iteration: 146 loss: 1.35804\n",
      "iteration: 147 loss: 1.53446\n",
      "iteration: 148 loss: 1.29636\n",
      "iteration: 149 loss: 1.40223\n",
      "iteration: 150 loss: 1.03207\n",
      "iteration: 151 loss: 1.51791\n",
      "iteration: 152 loss: 1.22962\n",
      "iteration: 153 loss: 1.67982\n",
      "iteration: 154 loss: 1.86377\n",
      "iteration: 155 loss: 1.53347\n",
      "iteration: 156 loss: 0.785148\n",
      "iteration: 157 loss: 1.07832\n",
      "iteration: 158 loss: 0.899532\n",
      "iteration: 159 loss: 0.747019\n",
      "iteration: 160 loss: 1.22438\n",
      "iteration: 161 loss: 1.50366\n",
      "iteration: 162 loss: 1.42868\n",
      "iteration: 163 loss: 1.24544\n",
      "iteration: 164 loss: 1.02051\n",
      "iteration: 165 loss: 1.36089\n",
      "iteration: 166 loss: 1.38521\n",
      "iteration: 167 loss: 1.35465\n",
      "iteration: 168 loss: 1.38128\n",
      "iteration: 169 loss: 1.68808\n",
      "iteration: 170 loss: 1.08243\n",
      "iteration: 171 loss: 2.4159\n",
      "iteration: 172 loss: 1.08011\n",
      "iteration: 173 loss: 1.3579\n",
      "iteration: 174 loss: 0.959595\n",
      "iteration: 175 loss: 1.28495\n",
      "iteration: 176 loss: 0.96873\n",
      "iteration: 177 loss: 0.887935\n",
      "iteration: 178 loss: 0.584852\n",
      "iteration: 179 loss: 1.25813\n",
      "iteration: 180 loss: 0.655731\n",
      "iteration: 181 loss: 1.34687\n",
      "iteration: 182 loss: 1.04896\n",
      "iteration: 183 loss: 0.893779\n",
      "iteration: 184 loss: 0.795601\n",
      "iteration: 185 loss: 0.86368\n",
      "iteration: 186 loss: 1.00966\n",
      "iteration: 187 loss: 1.2277\n",
      "iteration: 188 loss: 1.22088\n",
      "iteration: 189 loss: 0.883971\n",
      "iteration: 190 loss: 1.37438\n",
      "iteration: 191 loss: 1.04577\n",
      "iteration: 192 loss: 1.22142\n",
      "iteration: 193 loss: 1.34987\n",
      "iteration: 194 loss: 0.977561\n",
      "iteration: 195 loss: 0.663161\n",
      "iteration: 196 loss: 1.73513\n",
      "iteration: 197 loss: 1.18955\n",
      "iteration: 198 loss: 1.30337\n",
      "iteration: 199 loss: 1.37195\n",
      "iteration: 200 loss: 0.984317\n",
      "learned W:\n",
      "[[ 2.55968857  3.23223662  4.23552895]\n",
      " [ 4.25433588  5.2582016   6.13617706]]\n",
      "learned b:\n",
      "[ 10.10518646  11.22683716  12.12639618]\n"
     ]
    }
   ],
   "source": [
    "problem_12()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
